{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31389fae-9b67-4627-9213-7f0eff67ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path(\"/Users/dheerajkumar/Developer/AI/Bgrad\")  # <-- adjust this\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from core import MLP, Layer, Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49b81c74-e541-43bb-9f57-a491c95d5088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9172832611642332, grad=0),\n",
       " Value(data=0.9732894209471539, grad=0),\n",
       " Value(data=0.15447838456004526, grad=0),\n",
       " Value(data=0.15902144200615353, grad=0),\n",
       " Value(data=-0.2806854835092665, grad=0),\n",
       " Value(data=-0.46252997610972746, grad=0),\n",
       " Value(data=0.2968322365690579, grad=0),\n",
       " Value(data=0.35743267269281964, grad=0),\n",
       " Value(data=-0.8100832297239737, grad=0),\n",
       " Value(data=-0.9733086877116657, grad=0),\n",
       " Value(data=-0.7888068323239157, grad=0),\n",
       " Value(data=-0.24943795254346002, grad=0),\n",
       " Value(data=0.6833196631568894, grad=0),\n",
       " Value(data=-0.43419788330019937, grad=0),\n",
       " Value(data=0.5095567392400762, grad=0),\n",
       " Value(data=-0.7071475229029589, grad=0),\n",
       " Value(data=0.9108396227487516, grad=0),\n",
       " Value(data=0.8645257420737162, grad=0),\n",
       " Value(data=0.3624934755314242, grad=0),\n",
       " Value(data=-0.6098446680804972, grad=0),\n",
       " Value(data=0.11315491475020178, grad=0),\n",
       " Value(data=-0.5639976042498132, grad=0),\n",
       " Value(data=0.8806210953032967, grad=0),\n",
       " Value(data=-0.7297589692320643, grad=0),\n",
       " Value(data=-0.5276239695135909, grad=0),\n",
       " Value(data=-0.1662859353492523, grad=0),\n",
       " Value(data=-0.5432190685124711, grad=0),\n",
       " Value(data=0.7588986823907897, grad=0),\n",
       " Value(data=0.30545965708452805, grad=0),\n",
       " Value(data=0.6637581197002222, grad=0),\n",
       " Value(data=0.05706547030606712, grad=0),\n",
       " Value(data=-0.8780440471351763, grad=0),\n",
       " Value(data=-0.0392684452102503, grad=0),\n",
       " Value(data=0.9723200590018459, grad=0),\n",
       " Value(data=-0.4772329327351199, grad=0),\n",
       " Value(data=0.7573384936390684, grad=0),\n",
       " Value(data=-0.6587572164042776, grad=0),\n",
       " Value(data=0.6047500426227881, grad=0),\n",
       " Value(data=-0.17849563171812988, grad=0),\n",
       " Value(data=0.36763631187149515, grad=0),\n",
       " Value(data=-0.47928523705565107, grad=0)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from core.nn_module import MLP\n",
    "\n",
    "# Inputs\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "\n",
    "# Outputs for each set of inputs\n",
    "yexpected = [1.0, -1.0, -1.0, 1.0]\n",
    "\n",
    "# Creating a neural network with random parameters (weights and biases)\n",
    "nn = MLP(3, [4, 4, 1])\n",
    "\n",
    "# Parameters of the NN\n",
    "nn.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3a6ace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train this custom NN on a tiny dataset and simulate training & learning process in neural network.\n",
    "# Forward pass, backward pass, gradient descent\n",
    "\n",
    "# Step 1 - Forward pass with current parameters and Calculating the loss\n",
    "# Step 2 - Backward pass to evaluate gradient for each parameter Which is rate of change of loss with respect to the parameter\n",
    "# Step 3 - Learning of the network by adjusting parameters based on the gradient\n",
    "# Step 4 - Repeat the process for multiple iterations till we have very low loss and we'll stop at very low loss when we feel predictions are good with expected predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f113669e-44cd-4cf4-b46e-920ca8049ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.236248109706986\n",
      "1 2.3267802695278728\n",
      "2 1.2191835799595179\n",
      "3 0.6941541385089913\n",
      "4 0.3222721292375048\n",
      "5 0.062234464893389234\n",
      "6 0.05134559291568403\n",
      "7 0.04367208624491083\n",
      "8 0.037940040115972386\n",
      "9 0.033493015039957885\n",
      "10 0.02994466201071131\n",
      "11 0.0270500336253564\n",
      "12 0.024645717524855774\n",
      "13 0.022618393606693703\n",
      "14 0.020886995548054243\n",
      "15 0.019392011910069045\n",
      "16 0.01808878520609186\n",
      "17 0.016943163694071922\n",
      "18 0.015928594389760933\n",
      "19 0.01502412908962448\n",
      "20 0.014213025669588823\n",
      "21 0.013481747354162677\n",
      "22 0.01281923399891867\n",
      "23 0.012216362994552601\n",
      "24 0.011665544716396171\n",
      "25 0.011160414979009452\n",
      "26 0.010695598453621894\n",
      "27 0.010266524691110043\n",
      "28 0.009869283619665853\n",
      "29 0.009500510997794802\n",
      "30 0.009157296835551353\n",
      "31 0.008837111596554712\n",
      "32 0.008537746288339133\n",
      "33 0.008257263491329145\n",
      "34 0.0079939570704486\n",
      "35 0.007746318829015478\n",
      "36 0.0075130107514721125\n",
      "37 0.0072928417743771045\n",
      "38 0.007084748248630288\n",
      "39 0.006887777427871849\n",
      "40 0.006701073451263742\n",
      "41 0.006523865392861863\n",
      "42 0.006355457031482017\n",
      "43 0.0061952180595402405\n",
      "44 0.0060425765006959595\n",
      "45 0.00589701214718905\n",
      "46 0.005758050860772891\n",
      "47 0.00562525960782196\n",
      "48 0.005498242120854372\n",
      "49 0.005376635096382295\n",
      "50 0.005260104853484687\n",
      "51 0.005148344389415011\n",
      "52 0.005041070778405397\n",
      "53 0.00493802286799794\n",
      "54 0.004838959234037594\n",
      "55 0.004743656361146394\n",
      "56 0.004651907020266603\n",
      "57 0.004563518818872445\n",
      "58 0.004478312902836248\n",
      "59 0.00439612279180283\n",
      "60 0.004316793332360883\n",
      "61 0.004240179755374283\n",
      "62 0.004166146825608627\n",
      "63 0.00409456807330408\n",
      "64 0.004025325098649656\n",
      "65 0.003958306941234825\n",
      "66 0.003893409507521319\n",
      "67 0.0038305350502159437\n",
      "68 0.003769591694149821\n",
      "69 0.0037104930038998384\n",
      "70 0.003653157588937038\n",
      "71 0.0035975087425651918\n",
      "72 0.0035434741113312997\n",
      "73 0.0034909853919565656\n",
      "74 0.0034399780531576074\n",
      "75 0.0033903910800112914\n",
      "76 0.0033421667387646334\n",
      "77 0.0032952503602122177\n",
      "78 0.0032495901399567364\n",
      "79 0.003205136954041152\n",
      "80 0.003161844188592779\n",
      "81 0.003119667582255472\n",
      "82 0.003078565080306431\n",
      "83 0.003038496699461368\n",
      "84 0.00299942440246759\n",
      "85 0.0029613119816700164\n",
      "86 0.0029241249508116934\n",
      "87 0.002887830444398943\n",
      "88 0.0028523971240224668\n",
      "89 0.002817795091081404\n",
      "90 0.0027839958054065453\n",
      "91 0.0027509720093237817\n",
      "92 0.002718697656739121\n",
      "93 0.002687147846863075\n",
      "94 0.0026562987622248463\n",
      "95 0.0026261276106565212\n",
      "96 0.002596612570954728\n",
      "97 0.0025677327419508433\n",
      "98 0.0025394680947437706\n",
      "99 0.0025117994278689945\n",
      "100 0.0024847083251958845\n",
      "101 0.0024581771163617815\n",
      "102 0.0024321888395672096\n",
      "103 0.002406727206569221\n",
      "104 0.002381776569723617\n",
      "105 0.0023573218909376213\n",
      "106 0.0023333487124053663\n",
      "107 0.002309843129008145\n",
      "108 0.0022867917622703413\n",
      "109 0.0022641817357696766\n",
      "110 0.0022420006519084084\n",
      "111 0.0022202365699584186\n",
      "112 0.0021988779852995877\n",
      "113 0.002177913809776832\n",
      "114 0.0021573333531059737\n",
      "115 0.002137126305264179\n",
      "116 0.0021172827198044693\n",
      "117 0.0020977929980385795\n",
      "118 0.0020786478740359533\n",
      "119 0.0020598384003902162\n",
      "120 0.0020413559347078458\n",
      "121 0.002023192126776832\n",
      "122 0.002005338906375587\n",
      "123 0.001987788471685529\n",
      "124 0.001970533278272542\n",
      "125 0.001953566028605319\n",
      "126 0.0019368796620801176\n",
      "127 0.001920467345523951\n",
      "128 0.0019043224641494552\n",
      "129 0.0018884386129368524\n",
      "130 0.0018728095884195683\n",
      "131 0.001857429380851637\n",
      "132 0.001842292166736625\n",
      "133 0.0018273923016985003\n",
      "134 0.0018127243136765135\n",
      "135 0.001798282896427204\n",
      "136 0.001784062903317226\n",
      "137 0.0017700593413921527\n",
      "138 0.0017562673657071644\n",
      "139 0.0017426822739059025\n",
      "140 0.0017292995010353383\n",
      "141 0.0017161146145846097\n",
      "142 0.0017031233097364914\n",
      "143 0.0016903214048213091\n",
      "144 0.001677704836962957\n",
      "145 0.0016652696579079486\n",
      "146 0.0016530120300283385\n",
      "147 0.0016409282224903766\n",
      "148 0.0016290146075807496\n",
      "149 0.0016172676571829842\n",
      "150 0.0016056839393970633\n",
      "151 0.0015942601152952405\n",
      "152 0.0015829929358079395\n",
      "153 0.0015718792387335808\n",
      "154 0.001560915945866751\n",
      "155 0.0015501000602391399\n",
      "156 0.0015394286634682617\n",
      "157 0.001528898913208999\n",
      "158 0.0015185080407034554\n",
      "159 0.0015082533484245884\n",
      "160 0.0014981322078096155\n",
      "161 0.0014881420570791492\n",
      "162 0.0014782803991383077\n",
      "163 0.0014685447995563474\n",
      "164 0.001458932884621221\n",
      "165 0.0014494423394660044\n",
      "166 0.001440070906264147\n",
      "167 0.0014308163824904394\n",
      "168 0.0014216766192450326\n",
      "169 0.0014126495196380442\n",
      "170 0.0014037330372317913\n",
      "171 0.0013949251745386955\n",
      "172 0.001386223981572332\n",
      "173 0.0013776275544495086\n",
      "174 0.0013691340340412333\n",
      "175 0.0013607416046706928\n",
      "176 0.001352448492856311\n",
      "177 0.0013442529660980035\n",
      "178 0.0013361533317050142\n",
      "179 0.0013281479356636165\n",
      "180 0.001320235161543162\n",
      "181 0.0013124134294389253\n",
      "182 0.0013046811949503285\n",
      "183 0.0012970369481932198\n",
      "184 0.0012894792128447762\n",
      "185 0.0012820065452199053\n",
      "186 0.0012746175333779506\n",
      "187 0.0012673107962583238\n",
      "188 0.0012600849828443437\n",
      "189 0.0012529387713539155\n",
      "190 0.001245870868456192\n",
      "191 0.0012388800085131337\n",
      "192 0.00123196495284529\n",
      "193 0.0012251244890206469\n",
      "194 0.0012183574301657938\n",
      "195 0.0012116626142986797\n",
      "196 0.001205038903682024\n",
      "197 0.001198485184196828\n",
      "198 0.0011920003647350515\n",
      "199 0.0011855833766109387\n"
     ]
    }
   ],
   "source": [
    "# Now we'll do Step 1 to Step 4 iteratively multiple times till we have very low loss\n",
    "# And we'll stop at very low loss when we feel predictions are good with expected predictions.\n",
    "interations = 200\n",
    "\n",
    "for k in range(interations):\n",
    "    # Forward pass to calculate the loss with current parameters\n",
    "    ypredicted = [nn(x) for x in xs]\n",
    "    loss = sum((ypred - yexp)**2 for ypred, yexp in zip(ypredicted, yexpected))\n",
    "\n",
    "    # Backward pass to calculate the gradient for each parameter\n",
    "    loss.backward()\n",
    "\n",
    "    # Speed/factor by which gradient moves in the opposite direction of gradient.\n",
    "    learning_rate = 0.1 \n",
    "\n",
    "    # Let's adjust parameters\n",
    "    # In the opposite direction of the gradient because\n",
    "    # If grad is +ve increasing parameter will increase loss and we want to decrease the loss.\n",
    "    # If grad is -ve increasing parameter will decrease loaa and we want to decrease the loss.\n",
    "    for p in nn.parameters():\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    # Clear old grads so backward pass in the next iteration results in the fresh gradients\n",
    "    nn.zero_grad()\n",
    "    \n",
    "    # Printing each iteration for tracking\n",
    "    print(k, loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73113b17-7db6-45a3-a0b2-f376834ba543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9850107919124951, grad=0),\n",
       " Value(data=-0.9913966746941082, grad=0),\n",
       " Value(data=-0.9779805535764464, grad=0),\n",
       " Value(data=0.9801082030278561, grad=0)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypredicted = [nn(x) for x in xs]\n",
    "ypredicted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
